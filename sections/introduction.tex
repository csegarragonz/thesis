\chapter{Introduction} \label{chap:introduction}

\section{Motivation}

Internet of Things (IoT) devices are more and more pervasive in our lives~\cite{Gartner2017}.
The number of devices owned per user is anticipated to increase by 26$\times$ by 2020~\cite{Barbosa2017}.
These devices continuously generate a large variety of data.
Notable examples include location-based sensors (\emph{e.g.}, GPS), inertial units (\emph{e.g.}, accelerometers, gyroscopes), weather stations, and, the focus of this paper, wearable sensors that monitor human-health data (\emph{e.g.}, blood pressure, heart rate, stress).

%Personalized health and medicine has the potential of being the next revolution in healthcare.
Also referred as P4 medicine (Predictive, Preventive, Personalized, and Participatory), personalized health uses this continuous stream of human-health data to make more targeted and effective diagnoses and treatments~\cite{Cumming2014}. 
Medical decisions are based on the predicted response of each particular user.
To implement personalized health ecosystems, a fleet of devices monitoring and processing each person's vital signs is of crucial importance.
%To implement this, larger amounts of data and complex processing pipelines are gradually being deployed, what generally leads to offloading computation to third-party cloud providers. When the data-in-motion are vital signs, protecting user's privacy becomes a topic of crucial importance. Furthermore, recent data protection regulations (\textit{e.g.}, GDPR~\cite{Voigt2017,gdpr}) stress the importance of protecting sensitive information against malicious attackers or untrusted cloud providers.

These devices usually have very restricted computing power and are typically very limited in terms of storage capacity.
Hence, this continuous processing of data must be off-loaded elsewhere, in particular for storage and processing purposes.
In doing so, one needs to take into account potential privacy and security threats that stem inherently from the nature of the data being generated and processed.
Cloud environments represent the ideal environment to offload such processing.
They allow deployers to hand-off the maintenance of the required infrastructure, with immediate benefit for instance in terms of scale-out with the workload. 

Processing privacy-sensitive data on untrusted cloud platforms presents a number of challenges.
A malicious (compromised) Cloud operator could observe and leak data, if no countermeasures are taken beforehand.
While there are software solutions that allow to operate on encrypted data (\emph{e.g.}, partial~\cite{Paillier1999} or full-homomorphic~\cite{Gentry2012} encryption), their current computational overhead makes them impractical in real-life scenarios~\cite{Gottel2018S}.

The recent introduction into the mass market of processors with embedded trusted execution environments (TEEs), \emph{e.g.}, Intel Software Guard Extensions (SGX)~\cite{Costan2016} (starting from processors with codename Skylake) or ARM TrustZone~\cite{trustzone}, offer a viable alternative to pure-software solutions.
TEEs protect code and data against several types of attacks, including a malicious underlying OS, software bugs, or threats from co-hosted applications.
The application's security boundary becomes the CPU itself.
The code is executed at near-native execution speeds inside enclaves of limited memory capacity.
All the major Infrastructure-as-a-Service providers (Google~\cite{gceskylake}, Amazon~\cite{amazonskylake}, IBM~\cite{ibm-sgx}, Microsoft~\cite{azureconfidential}) are nowadays offering nodes with SGX processors.

We focus on the specific use case of processing data streams generated by health-monitoring wearable devices on untrusted clouds with available SGX nodes.
%This setting addresses the fact that algorithms for analyzing cardiovascular signals are getting more complex and computation-intensive.
Algorithms for analyzing cardiovascular signals are getting more complex and computationally-intensive.
Thus, traditional signal-processing approaches~\cite{Kumar2016} have now evolved to more advanced solutions like deep neural networks~\cite{Xiong2018,VanZaen2019}.
This increase in computational expenditure has moved the processing towards centralized centers (\textit{i.e.}, the cloud) with bigger and more dynamic processing power. %when scaling up to a large fleet of wearable devices is needed.
In this work we present a system that computes in real time several metrics of the heart-rate variability (HRV) streaming from wearable sensors.
While existing stream processing solutions exist~\cite{spark-streaming-documentation,Havet2017}, they either lack support for SGX or, if they do support it, are tied to very specific programming frameworks and prevent adoption in industrial settings.

\section{Contributions}

In this section we highlight the contributions the work here presented makes, and also the components and resources we are given privileged access to.

The contributions of this thesis are twofold.
First, we design and implement a system that can process cardiac signals inside SGX enclaves in untrusted clouds.
Our design leverages \sgxspark, a stream processing system that exploits SGX to execute stream analytics inside TEEs (described in detail in \S\ref{chap:background}).
Note that our design is flexible enough to be used with different stream processing systems (as further described later), and other input data streams.
Second, we compare our system against the vanilla, non-secure Spark.
We perform an exhaustive assessment on the introduced overhead, and we conclude that the introduced slow-down factor is reasonable even for large datasets and high workloads.
What leads us to conclude that the technology is almost ready for production environments.
%Our evaluation shows that the current overhead of SGX is reasonable even for large datasets and for high-throughput workloads and that the technology is almost ready for production environments.

As introduced before, our design has \sgxspark in its processing core.
\sgxspark is a modification to Spark to run security sensitive code inside SGX (see \S\ref{sec:background:tech}).
It is an on-going project in the Large-Scale Data \& Systems group from the Imperial College London~\cite{lsds}.
We have been given early-access to the code in order to use it in our platform and provide a performance assessment.

\section{Document Structure}

The structure of the rest of this thesis is the following one.
In Chapter \ref{chap:background} we introduce the preliminaries required to follow the rest of the sections.
We divide it in \S\ref{sec:background:tech} where we introduce the concepts that surround the design, implementation, and deployment of the system.
And \S\ref{sec:background:med} where we introduce and motivate the envisioned use-case, and contextualize the data streams we will feed our streaming platform with.
In Chapter \ref{chap:related-work} we cover the state of the art for privacy preserving stream processing engines of medical data.
In Chapter \ref{chap:architecture} we first describe our system's architecture (\S\ref{sec:server}, \S\ref{sec:clients}) and then we go on to introduce our threat model (\S\ref{sec:threat}) and our system's known vulnerabilities (\S\ref{sec:vulnerabilities}).
Then, in Chapter \ref{chap:implementation}, we describe how the previously introduced architecture is implemented (\S\ref{sec:client-implementation}, \S\ref{sec:server-implementation}) and deployed (\S\ref{sec:deployment}).
A complete and exhaustive evaluation of the system is then presented in Chapter \ref{chap:evaluation}.
In particular, we first describe the evaluation context: hardware settings (\S\ref{sec:evaluation:hardware}), experiment configuration (\S\ref{sec:evaluation:experiments}), analyzed metrics (\S\ref{sec:evaluation:metrics}), and injected workloads (\S\ref{sec:evaluation:workload}).
To then present the obtained results in \S\ref{sec:evaluation:results}.
Lastly, further research lines and the thesis' conclusions are presented in Chapters \ref{chap:future-work} and \ref{chap:conclusion} respectively.
